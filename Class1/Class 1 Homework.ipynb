{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MCP Homework and Local LLM Implementation with Ollama & LangChain**\n",
    "*In this thrust we have four tasks, which will be enhancing MCP using and learning how to use two brilliant tools for LLM application.*\n",
    "\n",
    "Prework:  follow the instruction in slides.Implement the MCP in Local Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Using MCP to built Agent-like work-flow.\n",
    "\n",
    "**Task 1.1 MCP + Claude = Browser Automation**\\\n",
    "*use MCP + Claude to do browser automation tasks*\n",
    "step 1: Learn all kinds of cool MCP server\\\n",
    "[Introducing to more MCP server](https://github.com/punkpeye/awesome-mcp-servers)\n",
    "\n",
    "step 2: Install cline\\\n",
    "[cline](https://cline.bot/)\n",
    "\n",
    "step 3: Open Claude, follow the instruction in the MCP Configuration file(mcp_example.json). Use rave Search„ÄÅGitHub„ÄÅPuppeteer„ÄÅFilesystem„ÄÅSequential Thinking and Notion.\n",
    "\n",
    "Now, follow the instructions below to complete a small task for each plugin.\n",
    "\n",
    "1. üîç Use Brave Search to:\n",
    "Task: Search for ‚Äúlatest AI paper publication platforms‚Äù and list the top 3 search results with titles and URLs.\n",
    "Prompt in Claude:\n",
    "\"Use Brave Search to look up the latest AI paper publication platforms and return the top 3 results with title and link.\"\n",
    "\n",
    "2. üíº Use GitHub to:\n",
    "Task: Access one of your public repositories (e.g., my-cool-project) and list the 5 most recent commits.\n",
    "Prompt in Claude:\n",
    "\"Connect to my GitHub account using the MCP plugin and list the 5 latest commits from the repository my-cool-project.\"\n",
    "\n",
    "3. ü§ñ Use Puppeteer to:\n",
    "Task: Visit https://www.inference.ai/, take a full-page screenshot, and save it as example.png.\n",
    "Prompt in Claude:\n",
    "\"Use Puppeteer to go to https://www.inference.ai/ and capture a full-page screenshot saved as example.png.\"\n",
    "\n",
    "4. üíæ Use Filesystem to:\n",
    "Task: Create a new folder on your Desktop named mcp_test, and inside it, create a text file hello.txt containing ‚ÄúHello MCP!‚Äù.\n",
    "Prompt in Claude:\n",
    "\"Use Filesystem to create a folder named mcp_test on my Desktop and add a file hello.txt inside with the text 'Hello MCP!'.\"\n",
    "\n",
    "5. üß† Use Sequential Thinking to:\n",
    "Task: Think step-by-step about how to prepare for a technical interview and generate a preparation plan.\n",
    "Prompt in Claude:\n",
    "\"Use Sequential Thinking to create a step-by-step plan for preparing for a technical interview.\"\n",
    "\n",
    "6. üìù Use Notion to:\n",
    "Task: Create a new Notion page titled ‚ÄúMCP Automation Test‚Äù and log the results of all the tasks above.\n",
    "Prompt in Claude:\n",
    "\"Use the Notion plugin to create a new page titled 'MCP Automation Test' and write a summary of the tasks I just completed using each plugin.\"\n",
    "\n",
    "\n",
    "Advanced Task: Use Claude + Puppeteer to automatically visit a webpage, scrape table content, and save it locally (with the help of the Filesystem plugin).\n",
    "Project Management Workflow: Record the scraped and analyzed data into a Notion database, automatically generating documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Play with Ollama\n",
    "\n",
    "<img src=\"https://ollama.com/public/blog/meta-ollama-llama3.png\" alt=\"jupyter\" width=\"500\"/>\n",
    "\n",
    "***Ollama** is a **convenient** and **free** frameworkÔºådesigned for easy deployment and running of large language models (LLMs) locally.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 2.1: Install Ollama and run LLMs locally**  \n",
    "- refer to [Ollama](https://ollama.ai/) to complete installation.  \n",
    "- Run `ollama run llama2` from the command line to download and launch the `llama2` model.\n",
    "\n",
    "\n",
    "**Task 2.2: Using Ollama to call OpenAI API**\\\n",
    "*Ollama now has built-in compatibility with the OpenAI Chat Completions API, making it possible to use more tooling and applications with Ollama locally.*\n",
    "\n",
    "See official instruction belowÔºö\\\n",
    "[Ollama OpenAI Compatibility](https://ollama.com/blog/openai-compatibility)\\\n",
    "[Ollama OpenAI](https://github.com/ollama/ollama/blob/main/docs/openai.md)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UsageÔºö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To invoke Ollama‚Äôs OpenAI compatible API endpoint, use the same OpenAI format and change the hostname to http://localhost:11434:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curl Method:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curl http://localhost:11434/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"llama2\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello!\"\n",
    "            }\n",
    "        ]\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2020 World Series was played at various locations, including:\n",
      "\n",
      "* Dodger Stadium in Los Angeles, California (home stadium of the Los Angeles Dodgers)\n",
      "* Globe Life Field in Arlington, Texas (home stadium of the Tampa Bay Rays)\n",
      "* Petco Park in San Diego, California (home stadium of the San Diego Padres)\n",
      "\n",
      "The Series was played from October 21 to November 5, 2020.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama2\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In above examples, ‚ÄúOllama‚Äù is essentially acting as a local server that is compatible with the OpenAI API. In other words, the endpoint you‚Äôre calling‚Äîwhether via code or a cURL command‚Äîis not the official OpenAI endpoint at https://api.openai.com/v1/ but rather http://localhost:11434/v1/. The local process running on this port is Ollama.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Combining LangChain with Ollama‚Äôs local LLMs\n",
    "\n",
    "*LangChain simplifies every stage of the LLM application lifecycle.\n",
    "It unifies functional modules such as \"Prompt design\", \"Multi-round dialogue memory (Memory)\", \"External data retrieval (Retrieval)\" and \"Tools/Agents\" into a unified package. In this way, you do not need to manually manage each step of the language model call and data flow, and only need to focus on business logic.*\n",
    "\n",
    "LangChain‚ÄìOllama Documentation: [https://python.langchain.com/docs/integrations/llms/ollama](https://python.langchain.com/docs/integrations/llms/ollama)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*-PlFCd_VBcALKReO3ZaOEg.png\" alt=\"jupyter\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1 Reproduce practice in lecture using LCEL**\n",
    "\n",
    "*A chain is a sequence of steps or model calls connected together to achieve a larger task. Each step can involve retrieving information, transforming text, or invoking a language model in some way, and then passing its output on to the next step in the chain. This structure helps you build more complex workflows or pipelines using multiple actions in a simple, organized manner.*\n",
    "\n",
    "- what is LCEL? LCEL is a much simpler way to construct \"Chain\"\\\n",
    "[LCEL](https://python.langchain.com/docs/concepts/lcel/)\\\n",
    "why use it?\\\n",
    "[Why LCEL](https://python.langchain.com/v0.1/docs/expression_language/why/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code with a Ollama local modelÔºö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt: 'What is the capital of Germany?'\n",
      "Model answer: The capital of Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "# Example: Using LCEL to reproduce a \"Basic Prompting\" scenario\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama \n",
    "\n",
    "# 2. Define the prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is the capital of {topic}?\"\n",
    ")\n",
    "\n",
    "# 3. Define the model\n",
    "model = ChatOllama(model = \"llama2\")  # Using Ollama \n",
    "\n",
    "# 4. Chain the components together using LCEL\n",
    "chain = (\n",
    "    # LCEL syntax: use the pipe operator | to connect each step\n",
    "    {\"topic\": RunnablePassthrough()}  # Accept user input\n",
    "    | prompt                          # Transform it into a prompt message\n",
    "    | model                           # Call the model\n",
    "    | StrOutputParser()               # Parse the output as a string\n",
    ")\n",
    "\n",
    "# 5. Execute\n",
    "result = chain.invoke(\"Germany\")\n",
    "print(\"User prompt: 'What is the capital of Germany?'\")\n",
    "print(\"Model answer:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memorial University (MUN) has experienced a decline in total and international enrollment this fall semester, resulting in a loss of $6 million in revenue. This decrease is attributed to a 4.6% drop in total enrollment and a significant 23.5% decrease in international enrollment. As a result, MUN has already announced 20 layoffs earlier this year. Furthermore, international student enrolment in Atlantic Canada is down by 28% compared to the same period last year.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Try summarizing a longer article or passage of your choice.\n",
    "text = \"\"\"\n",
    "Memorial University (MUN) has reported a 4.6% decrease in total enrolment and a 23.5% decrease in international enrolment this fall compared to the 2024 fall semester resulting in $6M in lost revenue that could bring further cuts. MUN previously announced 20 layoffs back in July. Atlantic Canada international student enrolment is down 28% year-over-year.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the following text:\\n{text}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"text\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's the information we can extract from the text:\n",
      "\n",
      "* Name: John Doe\n",
      "* Age: 29\n",
      "* Location: San Francisco\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Extract the age and location from the same text.\n",
    "text = \"\"\"\n",
    "John Doe, a 29-year-old software engineer from San Francisco, recently joined OpenAI as a research scientist.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Extract the age and location from the following text:\\n{text}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"text\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Va a llover pronto.\"\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: Translate a different sentence to Spanish.\n",
    "text = \"It's going to rain soon.\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Translate the following text to Spanish:\\n{text}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"text\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In space, a robot\n",
      "Explores the cosmic vastness\n",
      "Gliding with grace\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5: Modify the prompt to write a poem about a robot exploring space.\n",
    "topic = \"robot exploring space\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write a haiku about {topic}.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(topic)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oh, oh, oh! *excitedly* Today, we're going to learn about something really cool called \"electron superposition\"! üöÄ\n",
      "\n",
      "So, do you know what an electron is? *pointing to a student* It's like a tiny little ball that's inside everything! ü§ñ And it can move around and do different things. Like when you touch something, your hand can feel it because of the electrons inside! üòç\n",
      "\n",
      "Well, did you know that when an electron is moving around, it can be in more than one place at the same time? *looks around* Yes, it's like magic! It's called \"superposition\" because it's like the electron is wearing a special suit that lets it be in two places at once. üé©\n",
      "\n",
      "But here's the really cool thing: when an electron is in two places at once, it can do different things in each place! *demonstrates with hands* Like, if you have two toy blocks, and one is red and one is blue, you can put them together and make a new color, like green! üåø\n",
      "\n",
      "And that's kind of like what electrons do when they're in superposition. They can be in two places at once, and they can do different things in each place! Isn't that amazing? üòÉ\n",
      "\n",
      "Now, I know this might sound a little confusing, but trust me, it's really cool! And the more we learn about electrons and their superposition, the more we can understand how the world works! *excitedly*\n",
      "\n",
      "So, let's play a game to practice what we learned. Can you imagine an electron moving around and being in two places at once? *looks around* Give it a try! üéâ\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6: Ask the model to explain a complex topic as if it were a kindergarten teacher.\n",
    "topic = \"electron superposition\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Explain {topic} as if you were a kindergarten teacher to their class.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(topic)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The probability of flipping a fair coin and landing on heads after 100 consecutive heads is (1/2)^100 = 0.368.\n",
      "\n",
      "To see why, consider the following argument:\n",
      "\n",
      "* The first flip has a probability of landing on heads of (1/2).\n",
      "* If the first flip lands on heads, then the second flip has a probability of landing on heads of (1/2) again, since the coin is fair and unbiased.\n",
      "* If the second flip lands on heads, then the third flip has a probability of landing on heads of (1/2), and so on.\n",
      "* Therefore, the probability of landing on heads after 100 consecutive heads is (1/2)^100 = 0.368.\n",
      "\n",
      "This makes sense intuitively, since if you've had 100 consecutive heads, it's unlikely that the next flip will also be heads. The probability of heads is still (1/2), but the probability of tails is now 1 - (1/2) = 0.632, so the total probability of landing on either heads or tails after 100 consecutive heads is 0.368.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 8: Pose a different math problem and ask for a step-by-step solution.\n",
    "problem = \"probability of flipping a fair coin and landing on heads 100 times given your previous 100 flips were all heads\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Solve the following problem and explain your reasoning: {problem}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"problem\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(problem)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( stage lights up, a comedian stands on stage with a witty smile )\n",
      "\n",
      "Comedian: Hey there folks! So, you know what's even funnier than making jokes about politics? Making jokes about politics while also being serious about data privacy! ( audience chuckles ) Yeah, I know, it's like the ultimate irony. But hey, someone's gotta do it.\n",
      "\n",
      "( gestures to a person in the audience ) You know who loves data privacy? That guy over there! ( points at a random person ) He's probably got his laptop locked away in a fortified bunker, just to be sure no one gets their hands on his sensitive information. Like, what could possibly go wrong with that? ( chuckles )\n",
      "\n",
      "But seriously, folks, data privacy is like the new \"Don't Ask, Don't Tell\". It's like, we know it's important, but do we actually do anything about it? ( audience nods in agreement ) I mean, have you seen the fine print on those privacy policy things? It's like trying to read a whole novel while tripping on acid. No wonder people just give up and let Facebook collect all their data without even realizing it! ( audience laughs )\n",
      "\n",
      "And don't even get me started on government surveillance. It's like, hello, have you guys heard of something called \"the Fourth Amendment\"? ( gestures to the audience ) It's this thing where the government can't just go around searching your house without a warrant. Oh wait, I forgot ‚Äì we live in a post-9/11 world now, so that doesn't apply anymore! ( audience laughs and chuckles )\n",
      "\n",
      "( holds up a mock-up of a government surveillance form ) Look at this, folks! ( points to the mock-up ) It's like, \"Hey, we want to collect all your data without any warrant or probable cause. Just sign here, please!\" And you know what? People are like, \"Sure thing! Can I get a stamp for my passport too?\" ( audience laughs and chuckles )\n",
      "\n",
      "( returns to the stage mic ) But seriously, folks, data privacy is important. It's like, if we don't protect our personal information, who will? ( audience nods in agreement ) I mean, have you seen the news lately? Government agencies are constantly leaking private data, and corporations are just as bad! They're like, \"Oh, we'll just collect all your data and sell it to the highest bidder. Your privacy is not our problem!\" ( audience chuckles )\n",
      "\n",
      "So, what can we do about it? ( looks around ) Well, first of all, we need to stop being so lazy and actually read those privacy policy things! ( holds up a mock-up of a privacy policy form ) It's like, 10 pages long, but trust me, it's worth it! And then we can start demanding better data protection from our government and corporations. ( audience nods in agreement )\n",
      "\n",
      "( returns to the stage mic ) And hey, if all else fails, we can always resort to good old-fashioned paranoia! ( holds up a mock-up of a tinfoil hat ) Just put this on your head, and no one can get your data! ( audience chuckles ) It's like, the ultimate privacy solution ‚Äì just make sure you don't trip over anything while wearing it! ( audience laughs )\n",
      "\n",
      "( stage lights dim, comedian exits stage )\n"
     ]
    }
   ],
   "source": [
    "# Exercise 9: Modify the system_prompt to make the assistant respond in a humorous tone. Observe how the responses change.\n",
    "style = \"a comedian who specializes in satirizing politics\"\n",
    "topic = \"importance of data privacy\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Describe {topic} in the style of {style}.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough(), \"style\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke({topic, style})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The operation you have provided is repeated the same calculation 3 times:\n",
      "\n",
      "{'times', '15', '7'} = 105\n",
      "{'times', '15', '7'} = 105\n",
      "{'times', '15', '7'} = 105\n",
      "\n",
      "So, the result of the operation is 105.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 10: Extend the agent by adding a function that multiplies two numbers. Test the agent with prompts that require multiplication.\n",
    "n1 = \"15\"\n",
    "n2 = \"7\"\n",
    "operation = \"times\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Perform the following arithmetic operation: {n1} {operation} {n2}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"n1\": RunnablePassthrough(), \"operation\": RunnablePassthrough(), \"n2\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke({n1, operation, n2})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Submission Requirements**  \n",
    "1. Complete all tasks.  \n",
    "2. Include screenshots of key outputs (e.g., model responses, agent computation results).\n",
    "\n",
    "- Advance WorkÔºöIntegrate the Ollama and Langchain tasks into **Gradio Web UI**, which will be useful for building Proxy AI-Agent interface translation with front-end, and demonstrate your work.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
