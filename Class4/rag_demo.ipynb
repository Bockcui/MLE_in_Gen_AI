{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Demo Notebook\n",
    "## Week 4: Retrieval-Augmented Generation with arXiv Papers\n",
    "\n",
    "This notebook demonstrates the complete RAG pipeline with example queries and retrieval results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "! pip install sentence-transformers faiss-cpu pymupdf arxiv tqdm fastapi uvicorn numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MIDTOWER\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rag_pipeline import ArXivRAGPipeline, download_arxiv_papers\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download arXiv Papers\n",
    "\n",
    "Download 50 recent cs.CL (Computation and Language) papers from arXiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 50 papers from cs.CL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 50/50 [01:00<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 50 papers to arxiv_papers\n",
      "Downloaded 50 papers\n",
      "First few papers: ['2511.10645v1', '2511.10643v1', '2511.10628v1', '2511.10621v1', '2511.10618v1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download papers (this may take 10-15 minutes)\n",
    "pdf_paths = download_arxiv_papers(\n",
    "    category='cs.CL',\n",
    "    max_results=50,\n",
    "    output_dir='arxiv_papers'\n",
    ")\n",
    "\n",
    "print(f\"Downloaded {len(pdf_paths)} papers\")\n",
    "print(f\"First few papers: {[Path(p).stem for p in pdf_paths[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize and Build the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "\n",
      "Processing 50 papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 50/50 [00:04<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 1074 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 34/34 [00:25<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index with dimension 384...\n",
      "Index built with 1074 vectors\n",
      "\n",
      "Pipeline ready with 1074 chunks from 50 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = ArXivRAGPipeline(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Process all papers\n",
    "pipeline.process_papers(\n",
    "    pdf_paths=pdf_paths,\n",
    "    chunk_size=512,\n",
    "    overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Index for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index to faiss_index.bin\n",
      "Saved chunks to chunks.json\n",
      "Saved metadata to metadata.json\n",
      "\n",
      "Index saved successfully!\n",
      "Total chunks indexed: 1074\n",
      "Total papers processed: 50\n"
     ]
    }
   ],
   "source": [
    "# Save the index and data\n",
    "pipeline.save_index(\n",
    "    index_path='faiss_index.bin',\n",
    "    chunks_path='chunks.json',\n",
    "    metadata_path='metadata.json'\n",
    ")\n",
    "\n",
    "print(\"\\nIndex saved successfully!\")\n",
    "print(f\"Total chunks indexed: {len(pipeline.chunks)}\")\n",
    "print(f\"Total papers processed: {len(set(m['paper'] for m in pipeline.metadata))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Queries - Retrieval Testing\n",
    "\n",
    "Let's test the retrieval system with various queries related to NLP and machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 8 example queries...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define some example queries\n",
    "example_queries = [\n",
    "    \"What are the latest advances in attention mechanisms for transformers?\",\n",
    "    \"How do large language models handle multilingual text?\",\n",
    "    \"What are the common evaluation metrics for machine translation?\",\n",
    "    \"Explain the architecture of BERT and its variants\",\n",
    "    \"What techniques are used for few-shot learning in NLP?\",\n",
    "    \"How does reinforcement learning apply to language generation?\",\n",
    "    \"What are the challenges in neural machine translation?\",\n",
    "    \"Describe methods for improving model interpretability\"\n",
    "]\n",
    "\n",
    "print(f\"Testing with {len(example_queries)} example queries...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the latest advances in attention mechanisms for transformers?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Rank 1] Distance: 1.0285\n",
      "Paper: 2511.10618v1\n",
      "Chunk ID: 21\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "vj) = A(qi, kl, vl) ∀i, j, l and necessarily that output activations from the attention layer are identical across all token embeddings. As norms and feedforward layers in the transformer are identical between token indices, outputs will be identical as well. This provides a clear reason as to why a...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 2] Distance: 1.0932\n",
      "Paper: 2511.10566v1\n",
      "Chunk ID: 12\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "that this distinctive behavior across a wide range of model architectures, spanning several vision and language datasets. Overall, our findings uncover a crucial connection on how layer normalization impacts learning and memorization in transformer models, with its broader impacts discussed in Appen...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 3] Distance: 1.1397\n",
      "Paper: 2511.10628v1\n",
      "Chunk ID: 4\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "compared to standard LayerNorm (Ba et al., 2016), particularly for large-scale language models (Takase et al., 2025; Touvron et al., 2023; Muennighoff et al., 2025). In addition, we apply QK-Norm (Dehghani et al., 2023; Muennighoff et al., 2025; Naseer et al., 2021), where layer normalization is inj...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = example_queries[0]\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = pipeline.search(query, k=3)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n[Rank {result['rank']}] Distance: {result['distance']:.4f}\")\n",
    "    print(f\"Paper: {result['metadata']['paper']}\")\n",
    "    print(f\"Chunk ID: {result['metadata']['chunk_id']}\")\n",
    "    print(f\"\\nText Preview (first 300 chars):\")\n",
    "    print(result['chunk'][:300] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: Multilingual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do large language models handle multilingual text?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Rank 1] Distance: 0.7551\n",
      "Paper: 2511.10229v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "separability in LangGPS es- sentially reflects the model’s own multilingual modeling capability. As multilingual capacities vary across models, LangGPS requires model-specific data selection, lacking the simplicity of a one-size-fits-all solution. Secondly, our ex- periments were conducted on LLaMA-...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 2] Distance: 0.7719\n",
      "Paper: 2511.10229v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "t-SNE (Van der Maaten and Hinton 2008) to visual- ize the representations of 200 sentences sampled from XNLI in parallel across English, Chinese and Arabic. As shown in Figure 5 (1) (2) (3), multilingual SFT leads to clearer linguistic boundaries and more compact clusters in the model’s representati...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 3] Distance: 0.7753\n",
      "Paper: 2511.10229v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "Despite these advances, the success of multilingual instruction tuning remains highly sensitive to the composi- tion and selection of the training data. Consequently, Data Selection has emerged as a critical strategy for balancing ef- fectiveness and efficiency in large-scale model training. Ex- ist...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = example_queries[1]\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = pipeline.search(query, k=3)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n[Rank {result['rank']}] Distance: {result['distance']:.4f}\")\n",
    "    print(f\"Paper: {result['metadata']['paper']}\")\n",
    "    print(f\"\\nText Preview (first 300 chars):\")\n",
    "    print(result['chunk'][:300] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the common evaluation metrics for machine translation?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Rank 1] Distance: 0.7956\n",
      "Paper: 2511.10338v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "a threshold of 0.4 was identified as a coverage gap. For each gap, we used SERP API to fetch and scrape new doc- uments. These documents are then used as a source for the document grounded generations. Table 25 shows broad as well as specific topic distributions (%) of the synthetic data generated. ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 2] Distance: 0.8092\n",
      "Paper: 2511.10591v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "the final weight. BERTScore: An embedding-based metric that measures the semantic similarity between the gen- erated and reference texts. ROUGE-L: A recall-oriented metric that mea- sures the longest common subsequence. 6 Results and Discussion 6.1 Performance Comparison Table 2 presents a comparati...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 3] Distance: 0.8593\n",
      "Paper: 2511.10229v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "separability in LangGPS es- sentially reflects the model’s own multilingual modeling capability. As multilingual capacities vary across models, LangGPS requires model-specific data selection, lacking the simplicity of a one-size-fits-all solution. Secondly, our ex- periments were conducted on LLaMA-...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = example_queries[2]\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = pipeline.search(query, k=3)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n[Rank {result['rank']}] Distance: {result['distance']:.4f}\")\n",
    "    print(f\"Paper: {result['metadata']['paper']}\")\n",
    "    print(f\"\\nText Preview (first 300 chars):\")\n",
    "    print(result['chunk'][:300] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4: BERT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain the architecture of BERT and its variants\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Rank 1] Distance: 1.0059\n",
      "Paper: 2511.10577v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "dual-encoder architecture—semantic and syntactic channels—which we enhance through DeBERTa integration. 3.2 Model Integration and Adjustments We evaluate the effect of replacing BERT in D2E2S with three DeBERTa vari- ants: DeBERTa V3-Base (86M parameters), DeBERTa V3-Large (304M parame- ters), and D...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 2] Distance: 1.0228\n",
      "Paper: 2511.10441v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "tests whether organizational benefits gen- eralize across alternation types rather than being phenomenon-specific optimizations. C Model Implementation C.1 Encoder Comparison We employ BERT (bert-base-multilingual-cased) as our primary encoder to emphasize contributions attributable to data organiza...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 3] Distance: 1.1316\n",
      "Paper: 2511.10441v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "prompt mimics our learn- ing objective using pretrained models and presents a more challenging task than standard multiple- 7Details on LLM configurations are in Appendix A.1. 4 10 1 10 2 10 3 Training Size (log scale) 0.70 0.75 0.80 0.85 0.90 0.95 1.00 F1 Score A: Model Architecture Comparison (BER...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = example_queries[3]\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = pipeline.search(query, k=3)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n[Rank {result['rank']}] Distance: {result['distance']:.4f}\")\n",
    "    print(f\"Paper: {result['metadata']['paper']}\")\n",
    "    print(f\"\\nText Preview (first 300 chars):\")\n",
    "    print(result['chunk'][:300] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 5: Few-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What techniques are used for few-shot learning in NLP?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Rank 1] Distance: 0.8441\n",
      "Paper: 2511.10354v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateu...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 2] Distance: 0.8516\n",
      "Paper: 2511.10192v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "prompt-based methods have become a crucial technique for improving performance. A typical prompt provided to an LLM generally includes multiple components: an instruction, database schema information, and the NL question posed by the user. To enhance the model’s generalization ability across differe...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Rank 3] Distance: 0.9953\n",
      "Paper: 2511.10441v1\n",
      "\n",
      "Text Preview (first 300 chars):\n",
      "central challenge in NLP. Recent work emphasizes how reorganizing raw text into structured or hierarchically arranged contexts can significantly enhance model learning (Liu et al., 2024; Sourati et al., 2024). “Structured data” often refers to knowledge graphs, table data, databases (Jiang et al., 2...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = example_queries[4]\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = pipeline.search(query, k=3)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n[Rank {result['rank']}] Distance: {result['distance']:.4f}\")\n",
    "    print(f\"Paper: {result['metadata']['paper']}\")\n",
    "    print(f\"\\nText Preview (first 300 chars):\")\n",
    "    print(result['chunk'][:300] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Results Summary\n",
    "\n",
    "Let's create a summary table showing all queries and their top results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RETRIEVAL SUMMARY TABLE\n",
      "================================================================================\n",
      "                                                          Query  Rank        Paper Distance                                                                                           Chunk Preview\n",
      "What are the latest advances in attention mechanisms for tra...     1 2511.10618v1   1.0285 vj) = A(qi, kl, vl) ∀i, j, l and necessarily that output activations from the attention layer are id...\n",
      "What are the latest advances in attention mechanisms for tra...     2 2511.10566v1   1.0932 that this distinctive behavior across a wide range of model architectures, spanning several vision a...\n",
      "What are the latest advances in attention mechanisms for tra...     3 2511.10628v1   1.1397 compared to standard LayerNorm (Ba et al., 2016), particularly for large-scale language models (Taka...\n",
      "      How do large language models handle multilingual text?...     1 2511.10229v1   0.7551 separability in LangGPS es- sentially reflects the model’s own multilingual modeling capability. As ...\n",
      "      How do large language models handle multilingual text?...     2 2511.10229v1   0.7719 t-SNE (Van der Maaten and Hinton 2008) to visual- ize the representations of 200 sentences sampled f...\n",
      "      How do large language models handle multilingual text?...     3 2511.10229v1   0.7753 Despite these advances, the success of multilingual instruction tuning remains highly sensitive to t...\n",
      "What are the common evaluation metrics for machine translati...     1 2511.10338v1   0.7956 a threshold of 0.4 was identified as a coverage gap. For each gap, we used SERP API to fetch and scr...\n",
      "What are the common evaluation metrics for machine translati...     2 2511.10591v1   0.8092 the final weight. BERTScore: An embedding-based metric that measures the semantic similarity between...\n",
      "What are the common evaluation metrics for machine translati...     3 2511.10229v1   0.8593 separability in LangGPS es- sentially reflects the model’s own multilingual modeling capability. As ...\n",
      "           Explain the architecture of BERT and its variants...     1 2511.10577v1   1.0059 dual-encoder architecture—semantic and syntactic channels—which we enhance through DeBERTa integrati...\n",
      "           Explain the architecture of BERT and its variants...     2 2511.10441v1   1.0228 tests whether organizational benefits gen- eralize across alternation types rather than being phenom...\n",
      "           Explain the architecture of BERT and its variants...     3 2511.10441v1   1.1316 prompt mimics our learn- ing objective using pretrained models and presents a more challenging task ...\n",
      "      What techniques are used for few-shot learning in NLP?...     1 2511.10354v1   0.8441 Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,...\n",
      "      What techniques are used for few-shot learning in NLP?...     2 2511.10192v1   0.8516 prompt-based methods have become a crucial technique for improving performance. A typical prompt pro...\n",
      "      What techniques are used for few-shot learning in NLP?...     3 2511.10441v1   0.9953 central challenge in NLP. Recent work emphasizes how reorganizing raw text into structured or hierar...\n"
     ]
    }
   ],
   "source": [
    "# Collect results for all queries\n",
    "summary_data = []\n",
    "\n",
    "for query in example_queries[:5]:  # First 5 queries for the report\n",
    "    results = pipeline.search(query, k=3)\n",
    "    \n",
    "    for result in results:\n",
    "        summary_data.append({\n",
    "            'Query': query[:60] + \"...\",\n",
    "            'Rank': result['rank'],\n",
    "            'Paper': result['metadata']['paper'][:30],\n",
    "            'Distance': f\"{result['distance']:.4f}\",\n",
    "            'Chunk Preview': result['chunk'][:100] + \"...\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RETRIEVAL SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Retrieval Quality\n",
    "\n",
    "Let's analyze some metrics about our retrieval system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Retrieval Distances by Query:\n",
      "(Lower distance = better match)\n",
      "\n",
      "What are the latest advances in attention mechanis... : 1.0871\n",
      "How do large language models handle multilingual t... : 0.7674\n",
      "What are the common evaluation metrics for machine... : 0.8214\n",
      "Explain the architecture of BERT and its variants... : 1.0534\n",
      "What techniques are used for few-shot learning in ... : 0.8970\n",
      "\n",
      "Papers Coverage:\n",
      "Total papers in index: 50\n",
      "Unique papers retrieved: 10\n",
      "Coverage: 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Calculate average distances for each query\n",
    "distance_analysis = {}\n",
    "\n",
    "for query in example_queries[:5]:\n",
    "    results = pipeline.search(query, k=3)\n",
    "    avg_distance = sum(r['distance'] for r in results) / len(results)\n",
    "    distance_analysis[query[:50]] = avg_distance\n",
    "\n",
    "print(\"\\nAverage Retrieval Distances by Query:\")\n",
    "print(\"(Lower distance = better match)\\n\")\n",
    "for query, avg_dist in distance_analysis.items():\n",
    "    print(f\"{query}... : {avg_dist:.4f}\")\n",
    "\n",
    "# Paper coverage analysis\n",
    "unique_papers_retrieved = set()\n",
    "for query in example_queries[:5]:\n",
    "    results = pipeline.search(query, k=3)\n",
    "    for result in results:\n",
    "        unique_papers_retrieved.add(result['metadata']['paper'])\n",
    "\n",
    "print(f\"\\nPapers Coverage:\")\n",
    "print(f\"Total papers in index: {len(set(m['paper'] for m in pipeline.metadata))}\")\n",
    "print(f\"Unique papers retrieved: {len(unique_papers_retrieved)}\")\n",
    "print(f\"Coverage: {len(unique_papers_retrieved) / len(set(m['paper'] for m in pipeline.metadata)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment with Different Parameters\n",
    "\n",
    "Let's test how different chunk sizes affect retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Strategy Options:\n",
      "\n",
      "Current configuration: 512 tokens with 50 token overlap\n",
      "\n",
      "Alternative configurations to experiment with:\n",
      "  - 256 tokens, 25 overlap: Smaller chunks, more precise\n",
      "  - 512 tokens, 50 overlap: Medium chunks (current)\n",
      "  - 1024 tokens, 100 overlap: Larger chunks, more context\n",
      "\n",
      "To experiment with different chunk sizes:\n",
      "1. Modify chunk_size and overlap parameters in process_papers()\n",
      "2. Rebuild the index\n",
      "3. Compare retrieval quality metrics\n"
     ]
    }
   ],
   "source": [
    "# Note: This is for experimentation - you would need to rebuild the index\n",
    "# with different parameters\n",
    "\n",
    "chunk_size_experiments = [\n",
    "    {\"size\": 256, \"overlap\": 25, \"description\": \"Smaller chunks, more precise\"},\n",
    "    {\"size\": 512, \"overlap\": 50, \"description\": \"Medium chunks (current)\"},\n",
    "    {\"size\": 1024, \"overlap\": 100, \"description\": \"Larger chunks, more context\"}\n",
    "]\n",
    "\n",
    "print(\"Chunking Strategy Options:\")\n",
    "print(\"\\nCurrent configuration: 512 tokens with 50 token overlap\")\n",
    "print(\"\\nAlternative configurations to experiment with:\")\n",
    "for exp in chunk_size_experiments:\n",
    "    print(f\"  - {exp['size']} tokens, {exp['overlap']} overlap: {exp['description']}\")\n",
    "\n",
    "print(\"\\nTo experiment with different chunk sizes:\")\n",
    "print(\"1. Modify chunk_size and overlap parameters in process_papers()\")\n",
    "print(\"2. Rebuild the index\")\n",
    "print(\"3. Compare retrieval quality metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load Previously Saved Index\n",
    "\n",
    "Demonstration of loading a pre-built index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Loaded index with 1074 vectors\n",
      "Loaded 1074 chunks\n",
      "Test query: 'transformer architecture'\n",
      "\n",
      "Top result from loaded index:\n",
      "Paper: 2511.10566v1\n",
      "Distance: 0.9588\n",
      "Preview: Eq. (33), we obtain z′ j = x′ j + FFN(x′ j), zj = xj + MHSA(xj). Hence, we can write gz′ i (from Eq. (10)) for the ith layer as follows: gz′ i = ∂L ∂z...\n"
     ]
    }
   ],
   "source": [
    "# Create a new pipeline instance\n",
    "loaded_pipeline = ArXivRAGPipeline(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Load the saved index\n",
    "loaded_pipeline.load_index(\n",
    "    index_path='faiss_index.bin',\n",
    "    chunks_path='chunks.json',\n",
    "    metadata_path='metadata.json'\n",
    ")\n",
    "\n",
    "# Verify it works\n",
    "test_query = \"transformer architecture\"\n",
    "results = loaded_pipeline.search(test_query, k=2)\n",
    "\n",
    "print(f\"Test query: '{test_query}'\")\n",
    "print(f\"\\nTop result from loaded index:\")\n",
    "print(f\"Paper: {results[0]['metadata']['paper']}\")\n",
    "print(f\"Distance: {results[0]['distance']:.4f}\")\n",
    "print(f\"Preview: {results[0]['chunk'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Building a complete RAG pipeline from scratch\n",
    "- Processing 50 arXiv papers into searchable chunks\n",
    "- Using FAISS for efficient similarity search\n",
    "- Testing retrieval quality with example queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
